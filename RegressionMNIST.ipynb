{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RegressionMNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJGWoL9B53IV"
      },
      "source": [
        "# **REGRESSION TASK**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qn1JVi-EWwh2"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "import random\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyNB6QiXSe7l"
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYmKtNg4X0z5"
      },
      "source": [
        "!wget -P regression_dataset https://gitlab.dei.unipd.it/gadaleta/nnld-2020-21-lab-resources/-/raw/master/homework_1_regression_dataset/train_data.csv\n",
        "!wget -P regression_dataset https://gitlab.dei.unipd.it/gadaleta/nnld-2020-21-lab-resources/-/raw/master/homework_1_regression_dataset/test_data.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ep24FixlA1Ej"
      },
      "source": [
        "# **Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dT6EOuNpA0E6"
      },
      "source": [
        "class Net(nn.Module): #NN definition: 3 hidden layers, ReLU activation function\n",
        "    \n",
        "    def __init__(self, Ni, Nh1, Nh2, Nh3, No):\n",
        "        super().__init__()\n",
        "        \n",
        "        print('Network initialized')\n",
        "        self.fc1 = nn.Linear(in_features=Ni, out_features=Nh1)\n",
        "        self.fc2 = nn.Linear(in_features=Nh1, out_features=Nh2)\n",
        "        self.fc3 = nn.Linear(in_features=Nh2, out_features=Nh3)\n",
        "        self.out = nn.Linear(in_features=Nh3, out_features=No)\n",
        "        self.act = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x, additional_out=False):\n",
        "        x = self.act(self.fc1(x))\n",
        "        x = self.act(self.fc2(x))\n",
        "        x = self.act(self.fc3(x))\n",
        "        x = self.out(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4UvepgLlsjI"
      },
      "source": [
        "# **Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94-ETE5XmDLs"
      },
      "source": [
        "class RegressionDataset(Dataset): #Dataset definition\n",
        "\n",
        "  def __init__(self, csv_file, transform=None): #initialization of the dataset\n",
        "    self.transform = transform\n",
        "    \n",
        "    with open(csv_file, 'r') as f: #reading of the file\n",
        "      lines = f.read().split('\\n')\n",
        "\n",
        "    lines.pop(0) #removing of the first line swince it's the title\n",
        "    self.data = []\n",
        "    count = 0\n",
        "    for line in lines:\n",
        "      count += 1\n",
        "      sample = line.split(',')\n",
        "      try:\n",
        "        self.data.append((float(sample[0]), float(sample[1]))) #check if there are any errors in the file (e. g. spaces)\n",
        "      except:\n",
        "        print()\n",
        "\n",
        "  def __len__(self): #lenght of the dataset\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx): #Function for retreiving elements. It receives a list of integers as input\n",
        "    elements = []\n",
        "\n",
        "    for elem in idx: #For each integer in the input list, retrives the element in that position\n",
        "      sample = self.data[elem]\n",
        "      if self.transform:\n",
        "          sample = self.transform(sample)\n",
        "      \n",
        "      elements.append(sample)\n",
        "    return elements #return list of selected elements\n",
        "\n",
        "class ToTensor(object):\n",
        "    \"\"\"Convert sample to Tensors.\"\"\"\n",
        "\n",
        "    def __call__(self, sample): #transforms the samples in a duple of tensors\n",
        "        x, y = sample\n",
        "        return (torch.tensor([x]).float(),\n",
        "                torch.tensor([y]).float())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfoFN-yGnmTh"
      },
      "source": [
        "composed_transform = transforms.Compose([ToTensor()])\n",
        "\n",
        "train_dataset = RegressionDataset('regression_dataset/train_data.csv', transform=composed_transform) #train dataset\n",
        "test_dataset = RegressionDataset('regression_dataset/test_data.csv', transform=composed_transform) #test dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdArleXN_V2-"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFnbM_0Hr1FO"
      },
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") #check if GPU is available \n",
        "print(f\"Training device: {device}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IIDThsS_uZZ"
      },
      "source": [
        "def initNet(): #function that initializes a new NN\n",
        "  torch.manual_seed(0)\n",
        "  Ni = 1 #number of input neurons\n",
        "  Nh1 = 100 #number of neurons of the first hidden layer\n",
        "  Nh2 = 200 #number of neurons of the second hidden layer\n",
        "  Nh3 = 100 #number of neurons of the third hidden layer\n",
        "  No = 1 #number of output neurons\n",
        "  net = Net(Ni, Nh1, Nh2, Nh3, No)\n",
        "  net.to(device)\n",
        "\n",
        "  return net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKtyTNK9BHaY"
      },
      "source": [
        "def get_optimizer(n, net, lr): #Function for choosing the optimizer: n = index for the list, net = network model, lr = learning rate\n",
        "  typeM = [optim.Adagrad(net.parameters(), lr=lr, weight_decay = 1e-3), optim.Adadelta(net.parameters(), lr = lr, weight_decay = 1e-3), optim.Adam(net.parameters(), lr=lr, weight_decay = 1e-3), optim.RMSprop(net.parameters(), lr=lr, weight_decay = 1e-3)]\n",
        "\n",
        "  return typeM[n]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhIR7s2Mb-bk"
      },
      "source": [
        "###########\n",
        "#K-Fold Cross Validation\n",
        "###########\n",
        "\n",
        "nIter = 3 #number of different set of hyperparameters to try\n",
        "model_set = {} #best hyperparameters \n",
        "loss_fn = nn.MSELoss() #MSE loss function\n",
        "\n",
        "for i in range(nIter): #for the number of different set\n",
        "  print('#################')\n",
        "  print(f'# Iter {i}')\n",
        "  print('#################')\n",
        "\n",
        "  kf = KFold(n_splits=5) #5-fold cross validation\n",
        "  num_epochs = random.randrange(2000, 5000, 20) #number of iteration for training the model\n",
        "  lr = random.uniform(0, 0.01) #learning rate\n",
        "  batch_num = [] #batch size\n",
        "  type_optimizer = random.randrange(0, 4) #type of optimizer (i. e. index in the dictionary for choosing the optimizer)\n",
        "  \n",
        "  train_loss_Fold = [] #list for saving the training loss for each fold\n",
        "  val_loss_Fold = [] #list for saving the validation loss for each fold\n",
        "\n",
        "  for train_index, val_index in kf.split(train_dataset): #for each fold\n",
        "    net = initNet()\n",
        "    optimizer = get_optimizer(type_optimizer, net, lr)\n",
        "    trainSet, valSet = train_dataset.__getitem__(train_index), train_dataset.__getitem__(val_index) #division in training set and validation set\n",
        "\n",
        "    if batch_num == []: #check if it's the first iteration\n",
        "      batch_num = random.randrange(4, len(trainSet) + 1)\n",
        "\n",
        "    trainSetX = DataLoader(trainSet, batch_size= batch_num, shuffle=True, num_workers=0) #dataloader of the training set\n",
        "    valSetX = DataLoader(valSet, batch_size=len(valSet), shuffle=True, num_workers=0) #dataloader of the validation set\n",
        "\n",
        "    train_loss= [] #list for saving the training loss at each epoch\n",
        "    val_loss= [] #list for saving the validation loss at each epoch\n",
        "\n",
        "    for epoch_num in range(num_epochs): #for each epoch\n",
        "\n",
        "      net.train() #training \n",
        "      for sample_batched in trainSetX: #for each batch\n",
        "        x_batch = sample_batched[0].to(device) #input elements\n",
        "        label_batch = sample_batched[1].to(device) #labels\n",
        "\n",
        "        out = net(x_batch) #output of the model\n",
        "\n",
        "        loss = loss_fn(out, label_batch) #loss of the model\n",
        "\n",
        "        net.zero_grad()\n",
        "        loss.backward() #backprobagation\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch_num == num_epochs - 1: #saving the loss at the last epoch\n",
        "          loss_batch = loss.detach().cpu().numpy()\n",
        "          train_loss.append(loss_batch)\n",
        "\n",
        "      net.eval() #evaluation\n",
        "      with torch.no_grad():\n",
        "        for sample_batched in valSetX: #for each batch\n",
        "          x_batch = sample_batched[0].to(device) #input elements\n",
        "          label_batch = sample_batched[1].to(device) #labels\n",
        "\n",
        "          out = net(x_batch) #output of the model\n",
        "\n",
        "          loss = loss_fn(out, label_batch) #loss of the model on the evaluation set\n",
        "\n",
        "          if epoch_num == num_epochs - 1: #saving the loss at the last epoch\n",
        "            loss_batch = loss.detach().cpu().numpy()\n",
        "            val_loss.append(loss_batch)\n",
        "  \n",
        "    train_loss = np.mean(train_loss) #mean loss of different batches\n",
        "    train_loss_Fold.append(train_loss)\n",
        "\n",
        "    val_loss = np.mean(val_loss) #mean loss of different batches\n",
        "    val_loss_Fold.append(val_loss)\n",
        "\n",
        "  train_loss = np.mean(train_loss_Fold) #mean of the loss of each fold\n",
        "  print(f\"AVERAGE TRAIN LOSS: {train_loss}\")\n",
        "\n",
        "  val_loss = np.mean(val_loss_Fold) #mean of the loss of each fold\n",
        "  print(f\"AVERAGE VAL LOSS: {np.mean(val_loss)}\")\n",
        "  val_loss_Fold.append(val_loss)\n",
        "\n",
        "  if len(model_set) == 0 or val_loss < model_set[\"loss\"]: #save the new best hyperparameter if the new validation losso is lower\n",
        "    model_set[\"num_epochs\"] = num_epochs\n",
        "    model_set[\"lr\"] = lr\n",
        "    model_set[\"num_batch\"] = batch_num\n",
        "    model_set[\"type_opt\"] = type_optimizer\n",
        "    model_set[\"loss\"] = val_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A14kAJ6lqzII"
      },
      "source": [
        "In the next cell you can choose to use whether the hyperparameters that I found to be good or the set of hyperparameters that you found during cross validation\n",
        "\n",
        "Set 'use_saved' equal to True if you want to use the set that I found"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLUdLf7q61zD"
      },
      "source": [
        "hyper_set = {} #set of the best hyperparameters\n",
        "use_saved = True #true if we want to use a set of hyperparameters that is already checked to be very good\n",
        "                #false if we want to use the new set of hyperparameters given by cross validation\n",
        "if use_saved:\n",
        "  hyper_set = {'lr': 0.0015, 'num_batch': 15, 'num_epochs': 1495, 'type_opt': 2}\n",
        "else:\n",
        "  hyper_set = {'lr': model_set[\"lr\"], 'num_batch': model_set[\"num_batch\"], 'num_epochs': model_set[\"num_epochs\"], 'type_opt': model_set[\"type_opt\"]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7xEcPIwDgWX"
      },
      "source": [
        "num_epochs = hyper_set[\"num_epochs\"] #number of epochs\n",
        "lr = hyper_set[\"lr\"] #larning rate\n",
        "batch_num = hyper_set[\"num_batch\"] #size of the batch\n",
        "type_optimizer = hyper_set[\"type_opt\"] #type of otpimizer\n",
        "\n",
        "train_loss_log = [] #list of training error at each epoch\n",
        "test_loss_log = [] #list of validation error at each epoch\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset.__getitem__(range(len(train_dataset))), batch_size=batch_num, shuffle=True, num_workers=0)\n",
        "\n",
        "net = initNet() #initialization of the nework\n",
        "loss_fn = nn.MSELoss() #loss function\n",
        "optimizer = get_optimizer(type_optimizer, net, lr) #optimizer\n",
        "\n",
        "for epoch_num in range(num_epochs): #for each epoch\n",
        "  print('#################')\n",
        "  print(f'# EPOCH {epoch_num}')\n",
        "  print('#################')\n",
        "\n",
        "  train_loss= []\n",
        "  net.train()\n",
        "  for sample_batched in train_dataloader: #for each batch\n",
        "    x_batch = sample_batched[0].to(device)\n",
        "    label_batch = sample_batched[1].to(device)\n",
        "\n",
        "    out = net(x_batch)\n",
        "\n",
        "    loss = loss_fn(out, label_batch)\n",
        "\n",
        "    net.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loss_batch = loss.detach().cpu().numpy()\n",
        "    train_loss.append(loss_batch)\n",
        "    \n",
        "  train_loss = np.mean(train_loss) #mean training loss of the batches\n",
        "  print(f\"AVERAGE TRAIN LOSS: {train_loss}\")\n",
        "  train_loss_log.append(train_loss)\n",
        "\n",
        "\n",
        "#Save the trained network\n",
        "\n",
        "#net_state_dict = net.state_dict()\n",
        "#torch.save(net_state_dict, 'Regression.torch')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1opSDnuM_eQ-"
      },
      "source": [
        "#######\n",
        "#Plot of the evolution of the training error\n",
        "#######\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.semilogy(train_loss_log, label='Train loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NHQS0SU2X5H"
      },
      "source": [
        "# **Testing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtEtHkc5rrIz"
      },
      "source": [
        "Set 'use_pre' equal to True in the next cell to load a pretrained network.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97eZ1xykZcRu"
      },
      "source": [
        "#Load a previus trained network\n",
        "use_pre = False\n",
        "\n",
        "if use_pre:\n",
        "  net = initNet() \n",
        "  # Load the state dict previously saved\n",
        "  net_state_dict = torch.load('Regression.torch')\n",
        "  # Update the network parameters\n",
        "  net.load_state_dict(net_state_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etIHcGPVu2tL"
      },
      "source": [
        "#######\n",
        "#Testing of the Model on Test Dataset\n",
        "#######\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset.__getitem__(range(len(test_dataset))), batch_size=len(test_dataset), shuffle=False, num_workers=0)\n",
        "\n",
        "val_loss= []\n",
        "net.eval() #evaluation\n",
        "with torch.no_grad(): #disable gradient tracking\n",
        "  for sample_batched in test_dataloader:\n",
        "    x_batch = sample_batched[0].to(device)\n",
        "    label_batch = sample_batched[1].to(device)\n",
        "\n",
        "    out = net(x_batch) #output of the network\n",
        "\n",
        "    loss = loss_fn(out, label_batch) #test error\n",
        "\n",
        "    loss_batch = loss.detach().cpu().numpy()\n",
        "    val_loss.append(loss_batch)\n",
        "\n",
        "  val_loss = np.mean(val_loss) #mean of the loss of the batches\n",
        "  print(f\"AVERAGE TEST LOSS: {np.mean(val_loss)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAJpqJlhu9cI"
      },
      "source": [
        "x_vec = np.array([x[0] for x in test_dataset.data]) #test input data\n",
        "y_vec = out.squeeze().cpu().numpy() #opredicted labels\n",
        "testLabel = np.array([pair[1] for pair in test_dataset.data]) #true labels\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "ax = sns.regplot(x = x_vec, y = testLabel, line_kws={\"color\": \"red\", \"linewidth\" : 1, \"label\" : \"True Model\"}, order=14, ci=None)\n",
        "plt.scatter(x_vec, y_vec, label='Network Output')\n",
        "plt.scatter(x_vec, testLabel, label='True Points')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGOU79FPKg0Y"
      },
      "source": [
        "# **Weights**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcJiy6UZKnfh"
      },
      "source": [
        "#First hidden layer\n",
        "h1_w = net.fc1.weight.data.cpu().numpy()\n",
        "\n",
        "# Second hidden layer\n",
        "h2_w = net.fc2.weight.data.cpu().numpy()\n",
        "\n",
        "# Third hidden layer\n",
        "h3_w = net.fc3.weight.data.cpu().numpy()\n",
        "\n",
        "# Output layer\n",
        "out_w = net.out.weight.data.cpu().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adI4Z8nDKutK"
      },
      "source": [
        "#######\n",
        "#Plotting of the Weights Histograms\n",
        "########\n",
        "\n",
        "fig, axs = plt.subplots(4, 1, figsize=(12,8))\n",
        "axs[0].hist(h1_w.flatten(), 50)\n",
        "axs[0].set_title('First hidden layer weights')\n",
        "axs[1].hist(h2_w.flatten(), 50)\n",
        "axs[1].set_title('Second hidden layer weights')\n",
        "axs[2].hist(h3_w.flatten(), 50)\n",
        "axs[2].set_title('Third hidden layer weights')\n",
        "axs[3].hist(out_w.flatten(), 50)\n",
        "axs[3].set_title('Output layer weights')\n",
        "[ax.grid() for ax in axs]\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38RTrlHeLM3U"
      },
      "source": [
        "# **Activations**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhZ8sYtFLL2j"
      },
      "source": [
        "def get_activation(layer, input, output):\n",
        "    global activation\n",
        "    activation = torch.sigmoid(output)\n",
        "\n",
        "#register hook \n",
        "hook_handle = net.fc3.register_forward_hook(get_activation)\n",
        "# net.fc2.register_forward_hook(get_activation)\n",
        "# net.fc1.register_forward_hook(get_activation)\n",
        "\n",
        "#rnalyze activations\n",
        "net = net.to(device)\n",
        "net.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    x1 = torch.tensor([test_dataset.data[0][0]]).float().to(device) #first element of the test dataset\n",
        "    y1 = net(x1)\n",
        "    z1 = activation\n",
        "    x2 = torch.tensor([50]).float().to(device) #element not in the test dataset\n",
        "    y2 = net(x2)\n",
        "    z2 = activation\n",
        "    x3 = torch.tensor([test_dataset.data[-1][0]]).float().to(device) #last element of the test dataset\n",
        "    y3 = net(x3)\n",
        "    z3 = activation\n",
        "\n",
        "#remove hook\n",
        "hook_handle.remove()\n",
        "\n",
        "### Plot activations\n",
        "fig, axs = plt.subplots(3, 1, figsize=(12,6))\n",
        "axs[0].stem(z1.cpu().numpy(), use_line_collection=True)\n",
        "axs[0].set_title('Last layer activations for input x=%.2f' % x1)\n",
        "axs[1].stem(z2.cpu().numpy(), use_line_collection=True)\n",
        "axs[1].set_title('Last layer activations for input x=%.2f' % x2)\n",
        "axs[2].stem(z3.cpu().numpy(), use_line_collection=True)\n",
        "axs[2].set_title('Last layer activations for input x=%.2f' % x3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}